{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:53:16.325541Z",
     "start_time": "2025-03-13T21:52:30.279624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for Gemma 3 \n",
    "!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
      "  Cloning https://github.com/huggingface/transformers (to revision v4.49.0-Gemma-3) to c:\\users\\roegn\\appdata\\local\\temp\\pip-req-build-w39nqjjn\n",
      "  Resolved https://github.com/huggingface/transformers to commit 1c0f782fe5f983727ff245c4c1b3906f9b99eec2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.50.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roegn\\pycharmprojects\\dummy\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10936736 sha256=89e3f56ebde91480a536521bdd29ac5323bc05b6cc82585aff1df9cca9a24777\n",
      "  Stored in directory: C:\\Users\\roegn\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-0cd5np97\\wheels\\2d\\ce\\b8\\6e4e5caf6da0accb4290166970863ea84901a5402cb15bfd94\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.49.0\n",
      "    Uninstalling transformers-4.49.0:\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "Successfully installed transformers-4.50.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\roegn\\AppData\\Local\\Temp\\pip-req-build-w39nqjjn'\n",
      "  Running command git checkout -q 1c0f782fe5f983727ff245c4c1b3906f9b99eec2\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:54:40.760173Z",
     "start_time": "2025-03-13T21:54:40.755012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:54:41.570118Z",
     "start_time": "2025-03-13T21:54:41.549901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# provided by the authors\n",
    "# taken from predictions by another model\n",
    "# manually checked for correctness by us (Wikipedia)\n",
    "# two added entries (Tyler Hoechlin, Angelina Jolie) due to one double in the original list and one unclear case; now len = 100\n",
    "\n",
    "year_map = {'Sasha Calle': '1995', 'Annie Murphy': '1986', 'Golshifteh Farahani': '1983', 'Kate Mara': '1983',\n",
    "            'Josh Hartnett': '1978', 'Jennifer Lawrence': '1990', 'Aaron Taylor-Johnson': '1990',\n",
    "            'Rebecca Ferguson': '1983', 'Monica Barbaro': '1990', 'Chris Hemsworth': '1983',\n",
    "\n",
    "            'Wes Anderson': '1969', 'Daniel Portman': '1992', 'Lily-Rose Depp': '1999', \"Myha'la Herrold\": '1996',\n",
    "            'Zendaya': '1996', 'Ezra Miller': '1992', 'Olga Kurylenko': '1979', 'Zazie Beetz': '1991',\n",
    "            'Arnold Schwarzenegger': '1947', 'Emilia Clarke': '1986',\n",
    "\n",
    "            'Jess Bush': '1992', 'Clara Rugaard': '1997', 'Molly Gordon': '1994', 'Isabel May': '2000',\n",
    "            'Hailee Steinfeld': '1996', 'Hannah Waddingham': '1974', 'Rory Culkin': '1989', 'Cobie Smulders': '1982',\n",
    "            'Harrison Ford': '1942',\n",
    "\n",
    "            'Tom Cruise': '1962', 'Carol Kane': '1952', 'Alexandra Daddario': '1986', 'Gal Gadot': '1985',\n",
    "            'Tom Holland': '1996', 'Hayley Atwell': '1982', 'Salma Hayek': '1966', 'Ana de Armas': '1988',\n",
    "            'Will Poulter': '1993', 'Anson Mount': '1973',\n",
    "\n",
    "            'Paapa Essiedu': '1990', 'Sam Hargrave': '1982', 'Margot Robbie': '1990', 'Nicolas Cage': '1964',\n",
    "            'Henry Cavill': '1983', 'Juno Temple': '1989', 'Cailee Spaeny': '1998', 'Treat Williams': '1951',\n",
    "\n",
    "            'Alexander Skarsgård': '1976', 'Rebecca Romijn': '1972', 'Monica Dolan': '1969', 'Anya Taylor-Joy': '1996',\n",
    "            'Sophia Lillis': '2002', 'Emmanuelle Vaugier': '1976', 'Aaron Paul': '1979', 'Elliot Page': '1987',\n",
    "            'Robin Tunney': '1972', 'Mike Faist': '1992',\n",
    "\n",
    "            'Tinatin Dalakishvili': '1991', 'Sarah Snook': '1987', 'Jenna Ortega': '2002', 'Zoe Saldana': '1978',\n",
    "            'Anjana Vasan': '1987', 'Ben Mendelsohn': '1969', 'Jeremy Allen White': '1991', 'Ayo Edebiri': '1995',\n",
    "            'Keanu Reeves': '1964', 'Pom Klementieff': '1986',\n",
    "\n",
    "            'Scarlett Johansson': '1984', 'Tornike Gogrichiani': '1986', 'James Cameron': '1954',\n",
    "            'Pedro Pascal': '1975', 'Kaley Cuoco': '1985', 'Samuel L. Jackson': '1948', 'Terri Ivens': '1967',\n",
    "            'Florence Pugh': '1996', 'Shea Whigham': '1969',\n",
    "    \n",
    "            'Kingsley Ben-Adir': '1986', 'Michael Keaton': '1951', 'Julian Sands': '1958', 'Christopher Nolan': '1970',\n",
    "            'Tom Hanks': '1956', 'Clint Eastwood': '1930', 'Gabriel Macht': '1972', 'Fabiana Udenio': '1964',\n",
    "            'Tom Bateman': '1989', 'Jack Champion': '2004',\n",
    "\n",
    "            'Jake Gyllenhaal': '1980', 'Leonardo DiCaprio': '1974', 'Jason Schwartzman': '1980',\n",
    "            'Grace Caroline Currey': '1996', 'Sydney Sweeney': '1997', 'Emily Rudd': '1993', 'Samuel Blenkin': '1996',\n",
    "            'James Marsden': '1973', 'Jesse Plemons': '1988', 'Alan Ritchson': '1982',\n",
    "\n",
    "            'Cillian Murphy': '1976', 'Meghan Markle': '1981', 'Tyler Hoechlin': '1987', 'Angelina Jolie': '1975'}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:54:59.282304Z",
     "start_time": "2025-03-13T21:54:59.272189Z"
    }
   },
   "source": [
    "# Parse command line arguments\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"CoT-Decoding with lightweight models\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                    help=\"Model ID to use (default: TinyLlama-1.1B)\")\n",
    "parser.add_argument(\"--use_ollama\", action=\"store_true\",\n",
    "                    help=\"Use Ollama instead of HuggingFace models\")\n",
    "parser.add_argument(\"--ollama_model\", type=str, default=\"llama2\",\n",
    "                    help=\"Ollama model to use (default: llama2)\")\n",
    "parser.add_argument(\"--ollama_url\", type=str, default=\"http://localhost:11434\",\n",
    "                    help=\"Ollama API URL (default: http://localhost:11434)\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"gsm8k\",\n",
    "                    choices=[\"gsm8k\", \"yearparity\",\"multiarith\", \"bbh\", \"custom\"],\n",
    "                    help=\"Dataset to evaluate on\")\n",
    "parser.add_argument(\"--split\", type=str, default=\"test\",\n",
    "                    help=\"Dataset split to evaluate on\")\n",
    "parser.add_argument(\"--top_k\", type=int, default=10,\n",
    "                    help=\"Number of alternative tokens to consider (default: 10)\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1,\n",
    "                    help=\"Batch size for inference\")\n",
    "parser.add_argument(\"--num_samples\", type=int, default=150,\n",
    "                    help=\"Number of samples to evaluate\")\n",
    "parser.add_argument(\"--custom_dataset_path\", type=str, default=\"\",\n",
    "                    help=\"Path to custom dataset\")\n",
    "parser.add_argument(\"--decode_method\", type=str, default=\"greedy\",\n",
    "                    choices=[\"greedy\", \"cot-decoding\"],\n",
    "                    help=\"Decoding method to use\")\n",
    "#args = parser.parse_args([\n",
    "#    \"--model\", \"TinyLlama/TinyLlama_v1.1\",\n",
    "#    \"--dataset\", \"gsm8k\",\n",
    "#args = parser.parse_args([\n",
    "#    \"--model\", \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "#    \"--dataset\", \"yearparity\"\n",
    "args = parser.parse_args([\n",
    "    \"--model\", \"Qwen/Qwen2.5-0.5B\",\n",
    "    \"--dataset\", \"yearparity\",\n",
    "])\n",
    "    # Add any other arguments you want to pass\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:00.198091Z",
     "start_time": "2025-03-13T21:55:00.176090Z"
    }
   },
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:00.891332Z",
     "start_time": "2025-03-13T21:55:00.888089Z"
    }
   },
   "source": [
    "# List of models that are known to work well with RTX 3050 Ti (4GB VRAM)\n",
    "LIGHTWEIGHT_MODELS = [\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"microsoft/phi-2\",\n",
    "    \"stabilityai/stablelm-3b-4e1t\",\n",
    "    \"google/gemma-2b\",\n",
    "    \"google/gemma-2b-it\",\n",
    "    \"google/gemma-2-2b\"\n",
    "    \"google/gemma-3-1b-pt\",\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",  # May require 8-bit quantization\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:01.367548Z",
     "start_time": "2025-03-13T21:55:01.364471Z"
    }
   },
   "source": [
    "# List of recommended Ollama models for RTX 3050 Ti\n",
    "OLLAMA_MODELS = [\n",
    "    \"deepseek\",\n",
    "    \"qwen2.5\",\n",
    "    \"llama2\",\n",
    "    \"phi\",\n",
    "    \"phi3\",\n",
    "    \"phi3:mini\",\n",
    "    \"mistral\",\n",
    "    \"mistral:7b-instruct-v0.2-q4_0\",\n",
    "    \"gemma:2b\",\n",
    "    \"gemma:2b-instruct\",\n",
    "    \"gemma2:2b\",\n",
    "    \"gemma3:1b\",\n",
    "    \"neural-chat\",\n",
    "    \"wizard-math:7b-q4_0\",\n",
    "    \"stablelm-zephyr\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:05.201023Z",
     "start_time": "2025-03-13T21:55:01.741900Z"
    }
   },
   "source": [
    "# Verify if the selected model is in the recommended list (if not using Ollama)\n",
    "if not args.use_ollama and args.model not in LIGHTWEIGHT_MODELS:\n",
    "    print(\n",
    "        f\"Warning: {args.model} is not in the list of recommended lightweight models.\")\n",
    "    print(f\"Recommended models for RTX 3050 Ti: {LIGHTWEIGHT_MODELS}\")\n",
    "    response = input(\"Do you want to continue? (y/n): \")\n",
    "    if response.lower() != \"y\":\n",
    "        exit()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: google/gemma-3-1b-pt is not in the list of recommended lightweight models.\n",
      "Recommended models for RTX 3050 Ti: ['DeepSeek/DeepSeek-R1:1.5B', 'Qwen/Qwen2.5-0.5B-Instruct', 'Qwen/Qwen2.5-0.5B', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'microsoft/phi-2', 'stabilityai/stablelm-3b-4e1t', 'google/gemma-2b', 'google/gemma-2b-it', 'google/gemma-2-2bgoogle/gemma-3-1b-pt', 'bigcode/starcoder2-3b', 'mistralai/Mistral-7B-v0.1']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:06.065422Z",
     "start_time": "2025-03-13T21:55:06.062158Z"
    }
   },
   "source": [
    "# Verify if the selected Ollama model is in the recommended list\n",
    "if args.use_ollama and args.ollama_model not in OLLAMA_MODELS:\n",
    "    print(\n",
    "        f\"Warning: {args.ollama_model} is not in the list of recommended Ollama models.\")\n",
    "    print(f\"Recommended Ollama models for RTX 3050 Ti: {OLLAMA_MODELS}\")\n",
    "    print(\"You can see available models by running 'ollama list' in terminal\")\n",
    "    response = input(\"Do you want to continue? (y/n): \")\n",
    "    if response.lower() != \"y\":\n",
    "        exit()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:06.530140Z",
     "start_time": "2025-03-13T21:55:06.523627Z"
    }
   },
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Load model and tokenizer with appropriate quantization\"\"\"\n",
    "    if args.use_ollama:\n",
    "        # For Ollama, we don't need to load a model, so we return placeholder objects\n",
    "        print(f\"Using Ollama with model: {args.ollama_model}\")\n",
    "\n",
    "        # Check if Ollama is running\n",
    "        try:\n",
    "            response = requests.get(f\"{args.ollama_url}/api/tags\")\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\"Ollama server returned non-200 status code\")\n",
    "            print(\"Ollama server is running and responding\")\n",
    "\n",
    "            # Try to list available models\n",
    "            models_response = requests.get(f\"{args.ollama_url}/api/tags\")\n",
    "            if models_response.status_code == 200:\n",
    "                available_models = [model[\"name\"]\n",
    "                                    for model in models_response.json().get(\"models\", [])]\n",
    "                print(f\"Available Ollama models: {available_models}\")\n",
    "\n",
    "                if args.ollama_model not in available_models:\n",
    "                    print(\n",
    "                        f\"Warning: Model '{args.ollama_model}' not found in Ollama.\")\n",
    "                    print(\"You may need to pull it first with: ollama pull\",\n",
    "                          args.ollama_model)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to Ollama server: {e}\")\n",
    "            print(\n",
    "                \"Make sure Ollama is running on your machine. You can start it by running 'ollama serve'\")\n",
    "            exit(1)\n",
    "\n",
    "        # We don't actually need a real tokenizer for Ollama, but we'll create a dummy object\n",
    "        class DummyTokenizer:\n",
    "            def __call__(self, text, return_tensors=None):\n",
    "                return {\"input_ids\": torch.tensor([[0]])}  # Dummy value\n",
    "\n",
    "            def decode(self, token_ids, skip_special_tokens=None):\n",
    "                return \"\"  # We won't actually use this\n",
    "\n",
    "            eos_token_id = 0  # Dummy value\n",
    "\n",
    "        return None, DummyTokenizer()\n",
    "    else:\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Load model with 8-bit quantization for larger models\n",
    "        if \"7B\" in model_name or \"13B\" in model_name:\n",
    "            print(\"Loading with 8-bit quantization for larger model\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                load_in_8bit=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "        # Enable model evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        return model, tokenizer"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:06.987908Z",
     "start_time": "2025-03-13T21:55:06.982022Z"
    }
   },
   "source": [
    "def load_data(dataset_name, split, num_samples, custom_path=\"\"):\n",
    "    \"\"\"Load dataset for evaluation\"\"\"\n",
    "    if dataset_name == \"custom\" and custom_path:\n",
    "        # Load custom dataset (expected format: JSONL with 'question' and 'answer' fields)\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            if custom_path.endswith('.csv'):\n",
    "                df = pd.read_csv(custom_path)\n",
    "            elif custom_path.endswith('.json') or custom_path.endswith('.jsonl'):\n",
    "                df = pd.read_json(custom_path, lines=True)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Custom dataset must be in CSV or JSONL format\")\n",
    "\n",
    "            questions = df['question'].tolist()[:num_samples]\n",
    "            if 'answer' in df.columns:\n",
    "                answers = df['answer'].tolist()[:num_samples]\n",
    "            else:\n",
    "                answers = [\"\"] * len(questions)\n",
    "\n",
    "            return {'question': questions, 'answer': answers}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading custom dataset: {e}\")\n",
    "            exit(1)\n",
    "\n",
    "    # Load standard datasets\n",
    "    if dataset_name == \"gsm8k\":\n",
    "        dataset = load_dataset(\"gsm8k\", \"main\", split=split)\n",
    "        \n",
    "    elif dataset_name == \"yearparity\":\n",
    "        problems = []\n",
    "        # with adjustments taken from supplementary code of Wang and Zhou (2024)\n",
    "        for key in year_map:\n",
    "            # creation of question\n",
    "            text = \"Was \" + key + \" born in an even or odd year?\"\n",
    "            year = int(year_map[key])\n",
    "            # creation of excepected answer\n",
    "            if year % 2 == 0:\n",
    "                problems.append([text, \"even\"])\n",
    "            else:\n",
    "                problems.append([text, \"odd\"])\n",
    "        \n",
    "        return problems\n",
    "    \n",
    "    elif dataset_name == \"multiarith\":\n",
    "        dataset = load_dataset(\n",
    "            \"ChilleD/MultiArith\", split=split)\n",
    "    elif dataset_name == \"bbh\":\n",
    "        # You can specify which BBH tasks to use\n",
    "        dataset = load_dataset(\n",
    "            \"lukaemon/bbh\", \"sports_understanding\", split=split)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported\")\n",
    "\n",
    "    # Limit the number of samples\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:07.359385Z",
     "start_time": "2025-03-13T21:55:07.354382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_question(question, model_name):\n",
    "    \"\"\"Format question based on model requirements\"\"\"\n",
    "    if args.use_ollama:\n",
    "        # Adjust prompt based on model\n",
    "        # Get base model name without tags\n",
    "        model = args.ollama_model.split(':')[0]\n",
    "\n",
    "        if model in [\"llama2\", \"llama3\"]:\n",
    "            return f\"<s>[INST] Q: {question} [/INST]\"\n",
    "        elif model in [\"mistral\"]:\n",
    "            return f\"<s>[INST] Q: {question} [/INST]\"\n",
    "        elif model in [\"phi\", \"phi2\", \"phi3\"]:\n",
    "            return f\"Q: {question}\\nA:\"\n",
    "        elif model in [\"gemma\"]:\n",
    "            return f\"<start_of_turn>user\\nQ: {question}<end_of_turn>\\n<start_of_turn>model\\nA:\"\n",
    "        elif \"wizard-math\" in model:\n",
    "            return f\"USER: Q: {question}\\nASSISTANT:\"\n",
    "        else:\n",
    "            # Generic format for other models\n",
    "            return f\"Q: {question}\\nA:\"\n",
    "    else:\n",
    "        # Different models have different prompt formats\n",
    "        if \"TinyLlama\" in model_name:\n",
    "            return f\"<|user|>\\nQ: {question}\\n<|assistant|>\\nA:\"\n",
    "        elif \"phi\" in model_name:\n",
    "            return f\"Q: {question}\\nA:\"\n",
    "        elif \"gemma\" in model_name:\n",
    "            return f\"<start_of_turn>user\\nQ: {question}<end_of_turn>\\n<start_of_turn>model\\nA:\"\n",
    "        elif \"mistral\" in model_name:\n",
    "            return f\"[INST] Q: {question} [/INST] A:\"\n",
    "        else:\n",
    "            # Default format\n",
    "            return f\"Q: {question}\\nA:\""
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:08.241764Z",
     "start_time": "2025-03-13T21:55:08.235763Z"
    }
   },
   "source": [
    "def extract_answer(response_text, dataset, decode_method):\n",
    "    import re\n",
    "    \"\"\"Extract the final answer from the generated response\"\"\"\n",
    "    # This is a simple implementation - you might need to adjust based on your model's output\n",
    "    \n",
    "    # if the model generates follow-up questions (introduced with \"Question: ...\"), delete those\n",
    "    if \"Question:\" in response_text:\n",
    "        response_text = response_text.split(\"Question:\")[0].strip()\n",
    "    \n",
    "    # different answer method for Year Parity-Task:\n",
    "    if dataset == \"yearparity\":\n",
    "        # in case of greedy decoding, use first occurrence of \"odd\" or \"even\"\n",
    "        index = 0\n",
    "        # in case of CoT-decoding, use last occurrence of \"odd\" or \"even\"\n",
    "        if decode_method == \"cot_decoding\":\n",
    "            index = -1\n",
    "        \n",
    "        \n",
    "        # identifying answer\n",
    "        found = re.findall(r\"\\W*even\\W*|\\W*odd\\W*\", response_text, flags=re.IGNORECASE)\n",
    "\n",
    "        # evaluation\n",
    "        if found:\n",
    "            return re.sub(r\"\\W\", \"\", found[index]).lower() # to lowercase to match with expected answer\n",
    "\n",
    "        else:\n",
    "            return \"No answer found\"\n",
    "    \n",
    "    try:\n",
    "        # Look for the answer after \"The answer is\" or similar phrases\n",
    "        phrases = [\"The answer is\", \"answer is\", \"final answer is\", \"= \"]\n",
    "        for phrase in phrases:\n",
    "            if phrase in response_text:\n",
    "                answer_part = response_text.split(phrase)[-1].strip()\n",
    "                # Extract the first number\n",
    "                import re\n",
    "                numbers = re.findall(r\"[-+]?\\d+[.,]{1}\\d+|[-+]?\\d+\", answer_part)\n",
    "                if numbers:\n",
    "                    # delete comma from number (in e.g. 26,000), because those cases might not be properly evaluated\n",
    "                   return re.sub(\",\", \"\", numbers[0])\n",
    "        \n",
    "        # If no clear answer format is found, return the last number in the text\n",
    "        numbers = re.findall(r\"[-+]?\\d+[.,]{1}\\d+|[-+]?\\d+\", response_text)\n",
    "        if numbers:\n",
    "            # delete comma from number\n",
    "            return re.sub(\",\", \"\", numbers[-1])\n",
    "\n",
    "        return \"No answer found\"\n",
    "    except:\n",
    "        return \"Error extracting answer\""
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:08.708344Z",
     "start_time": "2025-03-13T21:55:08.704414Z"
    }
   },
   "source": [
    "def ollama_generate(prompt, model_name, temperature=0.0, max_tokens=200):\n",
    "    \"\"\"Generate text using Ollama API\"\"\"\n",
    "    url = f\"{args.ollama_url}/api/generate\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"\")\n",
    "        else:\n",
    "            print(f\"Error from Ollama API: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Exception when calling Ollama API: {e}\")\n",
    "        return f\"Error: {str(e)}\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:09.188374Z",
     "start_time": "2025-03-13T21:55:09.183501Z"
    }
   },
   "source": [
    "def ollama_get_top_logprobs(prompt, model_name, top_k=10):\n",
    "    \"\"\"Get top logprobs for the next token using Ollama API\"\"\"\n",
    "    url = f\"{args.ollama_url}/api/generate\"\n",
    "\n",
    "    # We'll use a trick to get logprobs: generate just 1 token and request logprobs\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_tokens\": 1,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_ctx\": 2048,\n",
    "            \"top_k_return\": top_k\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the logprobs from the response\n",
    "            # Note: This is Ollama-specific and might change with API updates\n",
    "            if \"top_k_return\" in response.json():\n",
    "                return response.json()[\"top_k_return\"]\n",
    "            else:\n",
    "                # Fallback if logprobs aren't available\n",
    "                # We'll just return a dummy list with the first token\n",
    "                first_token = response.json().get(\"response\", \"\")\n",
    "                return [(first_token, 0.0)]\n",
    "        else:\n",
    "            print(f\"Error from Ollama API: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return [(f\"Error: {response.status_code}\", 0.0)]\n",
    "    except Exception as e:\n",
    "        print(f\"Exception when calling Ollama API: {e}\")\n",
    "        return [(f\"Error: {str(e)}\", 0.0)]"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:09.909480Z",
     "start_time": "2025-03-13T21:55:09.905479Z"
    }
   },
   "source": [
    "def greedy_decode(model, tokenizer, input_text, max_new_tokens=200):\n",
    "    \"\"\"Standard greedy decoding\"\"\"\n",
    "    if args.use_ollama:\n",
    "        return ollama_generate(input_text, args.ollama_model, temperature=0.0, max_tokens=max_new_tokens)\n",
    "    else:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        return response"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:10.346907Z",
     "start_time": "2025-03-13T21:55:10.325583Z"
    }
   },
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def calculate_confidence(logits: List[torch.Tensor], answer_ids: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the confidence score (Δ) as specified in the paper.\n",
    "\n",
    "    Args:\n",
    "        logits: List of logits for each decoding step\n",
    "        answer_ids: Tensor of token ids for the answer\n",
    "\n",
    "    Returns:\n",
    "        Confidence score (Δ)\n",
    "    \"\"\"\n",
    "    \n",
    "    confidence_sum = 0.0\n",
    "    valid_tokens = 0\n",
    "\n",
    "    for t, token_id in enumerate(answer_ids):\n",
    "        if t >= len(logits):\n",
    "            break\n",
    "            \n",
    "        token_logits = logits[t]\n",
    "        probs = torch.softmax(token_logits, dim=-1)\n",
    "\n",
    "        if probs.size(-1) > 1:\n",
    "            top_2_probs, _ = torch.topk(probs, min(2, probs.size(-1)))\n",
    "\n",
    "            if top_2_probs.size(0) > 1:\n",
    "                confidence_sum += (top_2_probs[0] - top_2_probs[1]).item()\n",
    "\n",
    "            else:\n",
    "                confidence_sum += 1.0  # Max confidence if there's only one token\n",
    "\n",
    "        else:\n",
    "            confidence_sum += 1.0  # Max confidence if there's only one token\n",
    "            \n",
    "        valid_tokens += 1\n",
    "\n",
    "    return confidence_sum / valid_tokens if valid_tokens > 0 else 0.0\n",
    "\n",
    "\n",
    "def cot_decode(model, tokenizer, input_text, top_k=10, max_new_tokens=200, aggregate_paths=False):\n",
    "\n",
    "    \"\"\"Implementation of CoT-decoding as per the paper\"\"\"\n",
    "\n",
    "    if args.use_ollama:\n",
    "        \n",
    "        # Step 1: Get the top-k tokens (or completions) for the first position\n",
    "        paths = []\n",
    "        try:\n",
    "\n",
    "            # First, try to get top logprobs if the API supports it\n",
    "            top_logprobs = ollama_get_top_logprobs(input_text, args.ollama_model, top_k)\n",
    "\n",
    "            if len(top_logprobs) > 0 and not top_logprobs[0][0].startswith(\"Error\"):\n",
    "                \n",
    "                # Use the returned logprobs to create different starting points\n",
    "                for token, logprob in top_logprobs:\n",
    "                    # Generate a completion starting with this token\n",
    "                    output = ollama_generate(\n",
    "                        input_text + token,\n",
    "                        args.ollama_model,\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=max_new_tokens-1\n",
    "                    )\n",
    "\n",
    "\n",
    "                    # Get the input token IDs to determine where the answer starts\n",
    "                    input_ids = tokenizer(\n",
    "                        input_text, return_tensors=\"pt\").input_ids\n",
    "                    generated_sequence = output.sequences[0]\n",
    "                    answer_ids = generated_sequence[len(input_ids[0]):]\n",
    "                    answer_text = tokenizer.decode(\n",
    "                        answer_ids, skip_special_tokens=True)\n",
    "\n",
    "                    # Calculate confidence score (Δ)\n",
    "                    confidence = calculate_confidence(\n",
    "                        output.scores, answer_ids)\n",
    "                    paths.append((answer_text, confidence))\n",
    "\n",
    "\n",
    "                # Return results based on aggregation preference\n",
    "                if aggregate_paths:\n",
    "                    return aggregate_paths_based_on_scores(paths)\n",
    "                else:\n",
    "                    # Return just the text\n",
    "                    return max(paths, key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                # Fallback: Generate multiple samples with higher temperature\n",
    "                print(\"Logprobs not available, using temperature sampling as fallback\")\n",
    "                top_completions = []\n",
    "                for _ in range(top_k):\n",
    "                    completion = ollama_generate(\n",
    "                        input_text,\n",
    "                        args.ollama_model,\n",
    "                        temperature=0.7,\n",
    "                        max_tokens=max_new_tokens\n",
    "                    )\n",
    "\n",
    "                    # Assign a dummy logprob (we'll use length as a proxy for confidence)\n",
    "\n",
    "                    dummy_logprob = len(completion) / 100  # Simple heuristic\n",
    "                    top_completions.append((completion, dummy_logprob))\n",
    "\n",
    "\n",
    "                # Step 3: Select the generation that contains reasoning\n",
    "                def contains_reasoning(text):\n",
    "                    # Simple heuristic: check if the text has calculations/steps\n",
    "                    reasoning_indicators = [\n",
    "                        \"First\", \"Step\", \"Let's\", \"I'll\", \"=\", \"+\", \"-\", \"*\", \"/\",\n",
    "                        \"calculate\", \"step\", \"think\", \"reason\", \"therefore\", \"if\", \"because\"\n",
    "                    ]\n",
    "                    return any(indicator in text for indicator in reasoning_indicators) and len(text) > 50\n",
    "\n",
    "\n",
    "                reasoning_generations = [\n",
    "                    gen for gen in top_completions if contains_reasoning(gen[0])]\n",
    "\n",
    "\n",
    "                if reasoning_generations:\n",
    "                    # Return the most probable reasoning path\n",
    "                    best_generation = max(\n",
    "                        reasoning_generations, key=lambda x: x[1])\n",
    "                    return best_generation[0]\n",
    "\n",
    "                else:\n",
    "                    # Fall back to the most probable generation\n",
    "                    return max(top_completions, key=lambda x: x[1])[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in CoT decoding with Ollama: {e}\")\n",
    "            # Fallback to standard greedy decoding\n",
    "            completion = ollama_generate(\n",
    "                input_text, args.ollama_model, max_tokens=max_new_tokens)\n",
    "            return completion\n",
    "\n",
    "    else:\n",
    "        # Original HuggingFace implementation\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Step 1: Get the top-k tokens for the first position\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs[\"input_ids\"])\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "\n",
    "        # Step 2: For each of the top-k first tokens, perform greedy decoding to get full paths\n",
    "        all_generations = []\n",
    "\n",
    "        for i in range(top_k):\n",
    "            first_token = top_k_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Concatenate the input with the first token\n",
    "            current_input = torch.cat(\n",
    "                [inputs[\"input_ids\"], first_token], dim=1)\n",
    "\n",
    "\n",
    "            # Perform greedy decoding for the rest of the sequence\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    current_input,\n",
    "                    max_new_tokens=max_new_tokens-1,  # -1 because we already have the first token\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "\n",
    "\n",
    "            generated_sequence = outputs.sequences[0]\n",
    "            answer_ids = generated_sequence[input_length:]\n",
    "            answer_text = tokenizer.decode(\n",
    "                answer_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "            # Calculate confidence score (Δ) if scores are available\n",
    "            if hasattr(outputs, 'scores') and outputs.scores:\n",
    "                confidence = calculate_confidence(outputs.scores, answer_ids)\n",
    "                all_generations.append((answer_text, confidence))\n",
    "            else:\n",
    "                # Fallback to using the initial token logit\n",
    "                all_generations.append(\n",
    "                    (answer_text, top_k_logits[0, i].item()))\n",
    "\n",
    "\n",
    "        # Step 3: Select the generation that contains reasoning (heuristic: longer and has calculation steps)\n",
    "        def contains_reasoning(text):\n",
    "            # Simple heuristic: check if the text has calculations/steps\n",
    "            reasoning_indicators = [\n",
    "                \"First\", \"Step\", \"Let's\", \"I'll\", \"=\", \"+\", \"-\",\"*\", \"/\",\n",
    "                \"calculate\", \"step\", \"think\", \"reason\", \"therefore\", \"if\", \"because\"\n",
    "            ]\n",
    "            return any(indicator in text for indicator in reasoning_indicators) and len(text) > 50\n",
    "\n",
    "\n",
    "        reasoning_generations = [\n",
    "            gen for gen in all_generations if contains_reasoning(gen[0])]\n",
    "\n",
    "\n",
    "        if reasoning_generations:\n",
    "            # Return the most probable reasoning path\n",
    "            best_generation = max(reasoning_generations, key=lambda x: x[1])\n",
    "            return best_generation[0]\n",
    "        \n",
    "        else:\n",
    "            # Fall back to the most probable generation\n",
    "            return max(all_generations, key=lambda x: x[1])[0]\n",
    "\n",
    "def aggregate_paths_based_on_scores(paths):\n",
    "    \"\"\"\n",
    "    Aggregate the generated paths based on confidence scores.\n",
    "    Returns the path with the highest confidence score.\n",
    "\n",
    "    Args:\n",
    "        paths: List of tuples (answer_text, confidence_score)\n",
    "\n",
    "    Returns:\n",
    "        String containing the answer with highest confidence\n",
    "    \"\"\"\n",
    "    if not paths:\n",
    "        return \"\"\n",
    "\n",
    "    # Sort paths by confidence score in descending order\n",
    "    sorted_paths = sorted(paths, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the answer with the highest confidence\n",
    "    return sorted_paths[0][0]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:10.881872Z",
     "start_time": "2025-03-13T21:55:10.877960Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:11.427906Z",
     "start_time": "2025-03-13T21:55:11.418952Z"
    }
   },
   "source": [
    "def evaluate(model, tokenizer, dataset, dataset_name, decode_method, top_k):\n",
    "    \"\"\"Evaluate the model on the dataset\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #print(dataset)\n",
    "    for i, item in enumerate(dataset):\n",
    "        # specifically for GSM8K\n",
    "        if dataset_name == \"gsm8k\":\n",
    "            #print(\"GSM8K\")\n",
    "            lines = item['answer'].split('\\n')\n",
    "     \n",
    "            if lines:\n",
    "                reference_answer = lines[-1].replace(\"#### \", '')\n",
    "                question =  item['question']\n",
    "                \n",
    "        # for MultiArith        \n",
    "        elif dataset_name == \"multiarith\":\n",
    "        \n",
    "           question = item['question']\n",
    "           reference_answer = item['final_ans']\n",
    "                \n",
    "        elif isinstance(dataset, list):\n",
    "            # for year parity\n",
    "            question = item[0]\n",
    "            reference_answer = item[1]\n",
    "            \n",
    "        elif isinstance(dataset, dict):\n",
    "            # For custom datasets\n",
    "            #print(\"Isinstance\", item)\n",
    "            question = dataset[\"question\"][i]\n",
    "            reference_answer = dataset['answer'][i] if dataset['answer'][i] else \"No reference\"\n",
    "        else:\n",
    "            # For HuggingFace datasets\n",
    "            if \"question\" in item:\n",
    "                question = item[\"question\"]\n",
    "            elif \"input\" in item:\n",
    "                question = item[\"input\"]\n",
    "            else:\n",
    "                raise ValueError(\"Dataset structure not supported\")\n",
    "\n",
    "            if \"answer\" in item:\n",
    "                reference_answer = item[\"answer\"]\n",
    "            else:\n",
    "                reference_answer = \"No reference\"\n",
    "\n",
    "        # Prepare the input\n",
    "        input_text = prepare_question(question, args.model)\n",
    "\n",
    "        # Get the prediction\n",
    "        if decode_method == \"greedy\":\n",
    "            response = greedy_decode(model, tokenizer, input_text)\n",
    "        else:  # cot-decoding\n",
    "            response = cot_decode(model, tokenizer, input_text, top_k)\n",
    "\n",
    "        # Extract the final answer\n",
    "        predicted_answer = extract_answer(response, dataset_name, decode_method)\n",
    "\n",
    "        # Save result\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"full_response\": response,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"reference_answer\": reference_answer\n",
    "        })\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        # Print progress\n",
    "        if (i + 1) % 5 == 0:\n",
    "            #elapsed = time.time() - start_time\n",
    "            print(f\"Processed {i+1}/{len(dataset)} examples ({elapsed:.2f}s)\")\n",
    "            # Print the last example\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Response: {response[:100]}...\")\n",
    "            print(f\"Predicted: {predicted_answer}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    return results, elapsed"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:12.054533Z",
     "start_time": "2025-03-13T21:55:12.051019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation for GSM8K dataset\n",
    "# taken from the original implementation of the paper\n",
    "\n",
    "def _is_float(s):\n",
    "  try:\n",
    "    float(s)\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "def is_correct(target, ans):\n",
    "  if _is_float(target) and _is_float(ans):\n",
    "    if abs(float(target) - float(ans)) <= 1e-5:\n",
    "      return True\n",
    "  elif str(target) == str(ans):\n",
    "    return True\n",
    "  return False"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:12.836144Z",
     "start_time": "2025-03-13T21:55:12.830144Z"
    }
   },
   "source": [
    "def save_results(results, timer, dataset_name, args):\n",
    "    \"\"\"Save evaluation results to a file\"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    # Create filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if args.use_ollama:\n",
    "        model_name = args.ollama_model.replace(':', '-')\n",
    "    else:\n",
    "        model_name = args.model.split(\"/\")[-1]\n",
    "\n",
    "    filename = f\"results/cot_decoding_{model_name}_{args.dataset}_{args.decode_method}_{timestamp}.json\"\n",
    "\n",
    "    # Calculate accuracy if reference answers are available\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #specific evaluation for GSM8K taken from the paper:\n",
    "    #print(dataset_name, type(dataset_name))\n",
    "    if dataset_name == \"gsm8k\":\n",
    "        print(\"Evaluation Method GSM8K\")\n",
    "        for result in results:\n",
    "            if result[\"reference_answer\"] != \"No reference\":\n",
    "                total += 1\n",
    "                if is_correct(result[\"predicted_answer\"], result[\"reference_answer\"]):\n",
    "                    correct += 1\n",
    "    else:\n",
    "        for result in results:\n",
    "            if result[\"reference_answer\"] != \"No reference\":\n",
    "                total += 1\n",
    "                # Very simple accuracy check - this should be improved for real evaluation\n",
    "                if str(result[\"predicted_answer\"]) in str(result[\"reference_answer\"]):\n",
    "                    correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else \"N/A\"\n",
    "\n",
    "    # Save metadata and results\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"model\": args.ollama_model if args.use_ollama else args.model,\n",
    "            \"dataset\": args.dataset,\n",
    "            \"decode_method\": args.decode_method,\n",
    "            \"top_k\": args.top_k,\n",
    "            \"num_samples\": args.num_samples,\n",
    "            \"answers_found\": total,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"time\": timer\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:55:13.993328Z",
     "start_time": "2025-03-13T21:55:13.990323Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def main():\n",
    "    # Check GPU info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(\n",
    "            f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(args.model)\n",
    "    print(args.num_samples)\n",
    "    # Load dataset\n",
    "    if args.dataset == \"custom\":\n",
    "        if not args.custom_dataset_path:\n",
    "            print(\"Error: --custom_dataset_path must be provided for custom datasets\")\n",
    "            exit(1)\n",
    "        dataset = load_data(args.dataset, args.split,\n",
    "                            args.num_samples, args.custom_dataset_path)\n",
    "    else:\n",
    "        dataset = load_data(args.dataset, args.split, args.num_samples)\n",
    "\n",
    "    print(f\"Loaded {len(dataset)} examples from {args.dataset}\")\n",
    "\n",
    "    # Evaluate\n",
    "    results, timer = evaluate(model, tokenizer, dataset, args.dataset,\n",
    "                       args.decode_method, args.top_k)\n",
    "\n",
    "    # Save results\n",
    "    save_results(results, timer, args.dataset, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T20:23:27.928602Z",
     "start_time": "2025-03-13T20:23:27.927096Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T20:20:16.929057Z",
     "start_time": "2025-03-13T20:20:16.929057Z"
    }
   },
   "source": [
    "## ollama run stablelm-zephyr\n",
    "# ollama run wizard-math:7b-q4_0\n",
    "# ollama run mistral:v0.1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
