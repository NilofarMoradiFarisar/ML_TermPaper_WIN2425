{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dec = 'abc'\n",
    "cleaned_dec.startswith('\\(\\frac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence(logits: List[torch.Tensor], answer_ids: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the confidence score (Δ) as specified in the paper.\n",
    "\n",
    "    Args:\n",
    "        logits: List of logits for each decoding step\n",
    "        answer_ids: Tensor of token ids for the answer\n",
    "\n",
    "    Returns:\n",
    "        Confidence score (Δ)\n",
    "    \"\"\"\n",
    "    confidence_sum = 0.0\n",
    "    valid_tokens = 0\n",
    "    for t, token_id in enumerate(answer_ids):\n",
    "        if t >= len(logits):\n",
    "            break\n",
    "        token_logits = logits[t]\n",
    "        probs = torch.softmax(token_logits, dim=-1)\n",
    "        if probs.size(-1) > 1:\n",
    "            top_2_probs, _ = torch.topk(probs, min(2, probs.size(-1)))\n",
    "            if top_2_probs.size(-1) > 1:\n",
    "                confidence_sum += (top_2_probs[-1]\n",
    "                                   [0] - top_2_probs[-1][1]).item()\n",
    "            else:\n",
    "                confidence_sum += 1.0  # Max confidence if there's only one token\n",
    "        else:\n",
    "            confidence_sum += 1.0  # Max confidence if there's only one token\n",
    "        valid_tokens += 1\n",
    "\n",
    "    return confidence_sum / valid_tokens if valid_tokens > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_paths_based_on_scores(paths: List[Tuple[str, float]]) -> Tuple[str, float]:\n",
    "    \"\"\"Aggregate multiple paths based on their confidence scores.\"\"\"\n",
    "    answer_scores = {}\n",
    "    for answer, delta in paths:\n",
    "        answer_scores[answer] = answer_scores.get(answer, 0) + delta\n",
    "    best_answer = max(answer_scores, key=answer_scores.get)\n",
    "    return best_answer, answer_scores[best_answer]\n",
    "\n",
    "\n",
    "def cot_decode(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    messages: List[Dict[str, str]],\n",
    "    k: int = 10,\n",
    "    num_beams: int = 1,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 1.0,\n",
    "    repetition_penalty: float = 1.0,\n",
    "    length_penalty: float = 1.0,\n",
    "    no_repeat_ngram_size: int = 0,\n",
    "    early_stopping: bool = False,\n",
    "    aggregate_paths: bool = False,\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Implement CoT-decoding for a given chat input.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face transformer model.\n",
    "        tokenizer: The associated tokenizer.\n",
    "        messages: List of chat messages in the format [{\"role\": \"user\", \"content\": \"...\"}]\n",
    "        k: The number of alternative tokens to consider at the first step.\n",
    "        num_beams: Number of beams for beam search.\n",
    "        max_new_tokens: Maximum number of new tokens to generate.\n",
    "        temperature: Sampling temperature.\n",
    "        top_p: Nucleus sampling probability.\n",
    "        repetition_penalty: Repetition penalty factor.\n",
    "        length_penalty: Length penalty factor.\n",
    "        no_repeat_ngram_size: Size of n-grams to avoid repeating.\n",
    "        early_stopping: Whether to stop generation when all beams are finished.\n",
    "        aggregate_paths: Whether to aggregate multiple paths.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the best path (or aggregated result) and its confidence score.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    # Use the chat template to format the input\n",
    "    if tokenizer.chat_template:\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        # Fallback for tokenizers without chat templates\n",
    "        input_text = \"\\n\".join(\n",
    "            [f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
    "        input_text += \"\\nassistant:\"\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    # Set pad_token_id if it's not set\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Get the top-k tokens for the first decoding step\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        first_token_logits = outputs.logits[0, -1, :]\n",
    "        top_k_logits, top_k_indices = torch.topk(first_token_logits, k)\n",
    "\n",
    "    paths = []\n",
    "    for idx in top_k_indices:\n",
    "        # Generate sequence starting with the selected token\n",
    "        start_ids = torch.cat(\n",
    "            [input_ids, idx.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "        start_mask = torch.cat([attention_mask, torch.ones(\n",
    "            (1, 1), dtype=torch.long, device=device)], dim=-1)\n",
    "\n",
    "        output = model.generate(\n",
    "            start_ids,\n",
    "            attention_mask=start_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            early_stopping=early_stopping,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "        generated_sequence = output.sequences[0]\n",
    "        answer_ids = generated_sequence[len(input_ids[0]):]\n",
    "        answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate confidence score (Δ)\n",
    "        confidence = calculate_confidence(output.scores, answer_ids)\n",
    "        paths.append((answer_text, confidence))\n",
    "\n",
    "    if aggregate_paths:\n",
    "        return aggregate_paths_based_on_scores(paths)\n",
    "    else:\n",
    "        return max(paths, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT Decoding:\n",
      " 1. **Determine the total number of students in the class:**\n",
      "   \\[\n",
      "   \\text{Total number of students} = 20\n",
      "   \\]\n",
      "\n",
      "2. **Calculate the number of students enrolled in contemporary dance:**\n",
      "   \\[\n",
      "   \\text{Contemporary dance students} = 20\\% \\text{ of } 20 = 0.20 \\times 20 = 4\n",
      "   \\]\n",
      "\n",
      "3. **Calculate the number of students remaining after removing the 4 from contemporary dance:**\n",
      "   \\[\n",
      "   \\text{Remaining students after contemporary dance} = 20 - 4 = 16\n",
      "   \\]\n",
      "\n",
      "4. **Calculate the number of students enrolled in jazz dance:**\n",
      "   \\[\n",
      "   \\text{Jazz dance students} = 25\\% \\text{ of } 16 = 0.25 \\times 16 = 4\n",
      "   \\]\n",
      "\n",
      "5. **Calculate the total number of students enrolled in jazz dance:**\n",
      "   \\[\n",
      "   \\text{Total jazz dance students} = 4 + 4 = 8\n",
      "   \\]\n",
      "\n",
      "6. **Calculate the number of students remaining after removing the jazz dance students:**\n",
      "   \\[\n",
      "   \\text{Remaining students after jazz dance} = 16 - 8 = 8\n",
      "   \\]\n",
      "\n",
      "7. **Calculate the number of students enrolled in hip-hop dance:**\n",
      "   \\[\n",
      "   \\text{Hip-hop dance students} = 100\\% - \\text{Contemporary dance} - \\text{Jazz dance} = 100\\% - 4 - 8 = 94\\%\n",
      "   \\]\n",
      "\n",
      "8. **Determine the number of students enrolled in hip-hop dance:**\n",
      "   \\[\n",
      "   \\text{Hip-hop dance students} = 94\\%\n",
      "   \\]\n",
      "\n",
      "Thus, the percentage of students enrolled in hip-hop dance is \\(\\boxed{94\\%}\\).\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?\"}\n",
    "]\n",
    "\n",
    "# Generate the response using CoT decoding\n",
    "print(f\"Using device: {get_device()}\")\n",
    "result, confidence = cot_decode(\n",
    "    model, tokenizer, messages, aggregate_paths=True, max_new_tokens=512)\n",
    "print(f\"CoT Decoding:\\n {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU (Python 3.11)",
   "language": "python",
   "name": "pytorch-gpu-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
