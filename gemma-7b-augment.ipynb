{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7473 problems from training set\n",
      "\n",
      "Available Ollama models: ['mistral:latest', 'meditron:latest', 'llava:7b', 'qwen2.5-coder:7b', 'deepseek-r1:7b', 'gemma:2b', 'llama3.2:3b', 'deepseek-r1:1.5b', 'llama3:latest']\n",
      "\n",
      "Comparing model responses for the first problem:\n",
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Error calling Ollama API: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "Results comparison:\n",
      "\n",
      "Model: gemma:2b\n",
      "Solution: **Step 1: Calculate the number of clips Natalia sold in April.**\n",
      "\n",
      "48 friends x 2 clips per friend = 96 clips\n",
      "\n",
      "**Step 2: Calculate the number of clips Natalia sold in May.**\n",
      "\n",
      "96 clips (April) x 0.5 = 48 clips\n",
      "\n",
      "**Step 3: Add the number of clips sold in April and May.**\n",
      "\n",
      "96 + 48 = 144\n",
      "\n",
      "Therefore, Natalia sold **144 clips** altogether in April and May.\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: llama2\n",
      "Solution: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "class OllamaGSM8K:\n",
    "    # Model-specific prompt templates\n",
    "    PROMPT_TEMPLATES = {\n",
    "        \"gemma:2b\": \"\"\"Solve this math problem step by step:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let's solve this step by step:\"\"\",\n",
    "\n",
    "        \"llama2\": \"\"\"[INST] You are a helpful math tutor. Solve this math problem step by step, showing all work clearly.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Solution: [/INST]\"\"\",\n",
    "\n",
    "        \"mistral\": \"\"\"<s>[INST] Solve this math problem step by step, showing your work clearly.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let's break this down: [/INST]\"\"\",\n",
    "\n",
    "        \"neural-chat\": \"\"\"### System: You are a skilled math tutor helping students solve math problems step by step.\n",
    "\n",
    "### User: Please solve this math problem:\n",
    "{question}\n",
    "\n",
    "### Assistant: I'll help you solve this step by step:\"\"\",\n",
    "\n",
    "        \"phi\": \"\"\"You are a math expert. Show the solution to this problem step by step.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Solution:\"\"\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, model=\"gemma:2b\", temperature=0.7, max_tokens=1024):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.api_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "        # Validate model selection\n",
    "        if model not in self.PROMPT_TEMPLATES:\n",
    "            available_models = list(self.PROMPT_TEMPLATES.keys())\n",
    "            print(\n",
    "                f\"Warning: Model {model} not in known models: {available_models}\")\n",
    "            print(\"Using default template. You may want to add a custom template.\")\n",
    "\n",
    "    def list_available_models(self) -> List[str]:\n",
    "        \"\"\"Get list of available models from Ollama.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "            if response.status_code == 200:\n",
    "                models = [model['name'] for model in response.json()['models']]\n",
    "                return models\n",
    "            return []\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(\"Error: Could not connect to Ollama API\")\n",
    "            return []\n",
    "\n",
    "    def generate_prompt(self, question: str) -> str:\n",
    "        \"\"\"Generate model-specific prompt.\"\"\"\n",
    "        template = self.PROMPT_TEMPLATES.get(\n",
    "            self.model,\n",
    "            # Default template if model not found\n",
    "            \"Question: {question}\\n\\nLet's solve this step by step:\"\n",
    "        )\n",
    "        return template.format(question=question)\n",
    "\n",
    "    def call_ollama(self, prompt: str) -> Optional[str]:\n",
    "        \"\"\"Make API call to Ollama with configurable parameters.\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": self.temperature,\n",
    "                        \"num_tokens\": self.max_tokens\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error calling Ollama API: {e}\")\n",
    "            return None\n",
    "\n",
    "    def solve_problem(self, question: str) -> Dict:\n",
    "        \"\"\"Solve a single GSM8K problem.\"\"\"\n",
    "        prompt = self.generate_prompt(question)\n",
    "        response = self.call_ollama(prompt)\n",
    "        return {\n",
    "            'question': question,\n",
    "            'solution': response,\n",
    "            'model': self.model,\n",
    "            'temperature': self.temperature\n",
    "        }\n",
    "\n",
    "    def batch_solve(self, questions: List[str], batch_size: int = 5) -> List[Dict]:\n",
    "        \"\"\"Solve multiple problems with delay between batches.\"\"\"\n",
    "        solutions = []\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch = questions[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}...\")\n",
    "\n",
    "            for question in batch:\n",
    "                solution = self.solve_problem(question)\n",
    "                solutions.append(solution)\n",
    "                time.sleep(1)  # Delay between problems\n",
    "\n",
    "            if i + batch_size < len(questions):\n",
    "                print(\"Waiting between batches...\")\n",
    "                time.sleep(5)  # Delay between batches\n",
    "\n",
    "        return solutions\n",
    "\n",
    "\n",
    "def compare_models(question: str, models: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Compare solutions from different models for the same problem.\"\"\"\n",
    "    if models is None:\n",
    "        models = [\"gemma:2b\", \"llama2\", \"mistral\", \"neural-chat\", \"phi\"]\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        solver = OllamaGSM8K(model=model)\n",
    "        solution = solver.solve_problem(question)\n",
    "        results.append(solution)\n",
    "        time.sleep(2)  # Delay between model calls\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load your local GSM8K dataset\n",
    "    try:\n",
    "        train_df = pd.read_csv(\n",
    "            'C:/Users/Nilofar/Desktop/ML_TermPaper_WIN2425/datasets/gsm8k_data/train.csv')\n",
    "        print(f\"Loaded {len(train_df)} problems from training set\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Please ensure your GSM8K dataset is in 'gsm8k_data/train.csv'\")\n",
    "        return\n",
    "\n",
    "    # Get list of available models\n",
    "    solver = OllamaGSM8K()\n",
    "    available_models = solver.list_available_models()\n",
    "    print(\"\\nAvailable Ollama models:\", available_models)\n",
    "\n",
    "    # Test with different models\n",
    "    test_question = train_df['question'].iloc[0]\n",
    "    print(\"\\nComparing model responses for the first problem:\")\n",
    "    print(\"Question:\", test_question)\n",
    "\n",
    "    # Test subset of models (adjust based on what you have installed)\n",
    "    test_models = [\"gemma:2b\", \"llama2\"]  # Add or remove models as needed\n",
    "    results_df = compare_models(test_question, test_models)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults comparison:\")\n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"\\nModel: {row['model']}\")\n",
    "        print(\"Solution:\", row['solution'])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Example of saving results\n",
    "    results_df.to_csv('model_comparison.csv', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU (Python 3.11)",
   "language": "python",
   "name": "pytorch-gpu-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
